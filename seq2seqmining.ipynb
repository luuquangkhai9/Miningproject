{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "execution": {
     "iopub.execute_input": "2024-12-09T07:28:42.637948Z",
     "iopub.status.busy": "2024-12-09T07:28:42.637354Z",
     "iopub.status.idle": "2024-12-09T12:19:25.243543Z",
     "shell.execute_reply": "2024-12-09T12:19:25.242023Z",
     "shell.execute_reply.started": "2024-12-09T07:28:42.637913Z"
    },
    "id": "FSywvF6fReJy",
    "outputId": "801c4e44-72ee-4a43-8d21-33867b592209",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "# Ignore warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Directory where models are saved\n",
    "checkpoint_dir = \"./vit5-finetuned-vietnews-3\"\n",
    "model_name = \"/kaggle/input/train2/kaggle/working/vit5-finetuned-vietnews-2/checkpoint-4000\"\n",
    "\n",
    "# Load the tokenizer (this doesn't change)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).cuda()\n",
    "\n",
    "# Load the dataset\n",
    "train_data = pd.read_csv(\"/kaggle/input/datasetkhai/articles_training.tsv\", sep=\"\\t\")\n",
    "print(f'Train data: {len(train_data)}')\n",
    "\n",
    "train_data, valid_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a custom dataset class\n",
    "class VietNewsDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_input_length=512, max_target_length=128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self.data.iloc[index]\n",
    "        input_text = \"vietnews: \" + item['content'] + \"</s>\"\n",
    "        target_text = item['tags']\n",
    "\n",
    "        # Tokenize input and target\n",
    "        inputs = self.tokenizer(\n",
    "            input_text, max_length=self.max_input_length,\n",
    "            padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        targets = self.tokenizer(\n",
    "            target_text, max_length=self.max_target_length,\n",
    "            padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": targets[\"input_ids\"].squeeze(0),\n",
    "        }\n",
    "\n",
    "train_dataset = VietNewsDataset(train_data, tokenizer)\n",
    "valid_dataset = VietNewsDataset(valid_data, tokenizer)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./vit5-finetuned-vietnews-3\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=500,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    gradient_accumulation_steps=2,\n",
    "    fp16=True,  # Enable mixed precision for faster training\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "# Save the fine-tuned model after training\n",
    "trainer.save_model(checkpoint_dir)\n",
    "\n",
    "# Optionally save tokenizer\n",
    "tokenizer.save_pretrained(checkpoint_dir)\n",
    "\n",
    "# Evaluate the model\n",
    "metrics = trainer.evaluate()\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-12-09T12:22:21.488621Z",
     "iopub.status.busy": "2024-12-09T12:22:21.487819Z",
     "iopub.status.idle": "2024-12-09T12:25:29.711256Z",
     "shell.execute_reply": "2024-12-09T12:25:29.710307Z",
     "shell.execute_reply.started": "2024-12-09T12:22:21.488588Z"
    },
    "id": "K8bC8Qz4GcDI",
    "outputId": "b5fc7ff4-122a-41be-ebc3-253c2f592e5b",
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from tqdm import tqdm\n",
    "class VietNewsDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_input_length=512, max_target_length=64):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self.data.iloc[index]\n",
    "        input_text = \"vietnews: \" + item['content'] + \"</s>\"\n",
    "        target_text = item['tags']\n",
    "\n",
    "        # Tokenize input and target\n",
    "        inputs = self.tokenizer(\n",
    "            input_text, max_length=self.max_input_length,\n",
    "            padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        targets = self.tokenizer(\n",
    "            target_text, max_length=self.max_target_length,\n",
    "            padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": targets[\"input_ids\"].squeeze(0),\n",
    "        }\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"/kaggle/working/vit5-finetuned-vietnews-3/checkpoint-2000\").cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/working/vit5-finetuned-vietnews-3/checkpoint-2000\")\n",
    "\n",
    "################################################### 100 samles\n",
    "test_data = pd.read_csv(\"/kaggle/input/dataset/train1.tsv\", sep=\"\\t\")\n",
    "test_data = test_data.head(300)\n",
    "test_dataset = VietNewsDataset(test_data, tokenizer)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "model.eval()\n",
    "results = []\n",
    "for batch in tqdm(test_dataloader, desc=\"Generating Predictions\"):\n",
    "    with torch.no_grad():\n",
    "        input_ids = batch[\"input_ids\"].cuda()\n",
    "        attention_mask = batch[\"attention_mask\"].cuda()\n",
    "\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=128,\n",
    "            num_beams=4,\n",
    "            no_repeat_ngram_size=3,\n",
    "        )\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    decoded_targets = tokenizer.batch_decode(batch[\"labels\"], skip_special_tokens=True)\n",
    "\n",
    "    for pred, target in zip(decoded_preds, decoded_targets):\n",
    "        results.append({\"Prediction\": pred, \"Target\": target})\n",
    "\n",
    "def calculate_accuracy(results):\n",
    "    total_correct = 0\n",
    "    total_target = 0\n",
    "    for idx, result in enumerate(tqdm(results, desc=\"Calculating Accuracy\")):\n",
    "        prediction_tags = set(result['Prediction'].split(','))\n",
    "        target_tags = set(result['Target'].split(','))\n",
    "\n",
    "        correct_matches = prediction_tags.intersection(target_tags)\n",
    "        num_correct_matches = len(correct_matches)\n",
    "\n",
    "        total_correct += num_correct_matches\n",
    "        total_target += len(target_tags)\n",
    "\n",
    "        accuracy = num_correct_matches / len(target_tags) * 100\n",
    "        print(prediction_tags)\n",
    "        print(target_tags)\n",
    "        print(f'Local Accuracy: {(num_correct_matches/len(target_tags)):.2f}')\n",
    "\n",
    "    print(f'Overall Accuracy: {(total_correct/total_target):.2f}')\n",
    "\n",
    "calculate_accuracy(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ufUl1JAqAmj"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "class VietNewsDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_input_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self.data.iloc[index]\n",
    "        input_text = \"vietnews: \" + item['content'] + \"</s>\"\n",
    "\n",
    "        # Tokenize input\n",
    "        inputs = self.tokenizer(\n",
    "            input_text, max_length=self.max_input_length,\n",
    "            padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n",
    "        }\n",
    "\n",
    "test_data = pd.read_csv('testall.tsv', sep='\\t')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/working/vit5-finetuned-vietnews-3/checkpoint-2000\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"/kaggle/working/vit5-finetuned-vietnews-3/checkpoint-2000\").cuda()\n",
    "\n",
    "test_dataset = VietNewsDataset(test_data, tokenizer)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "for batch in tqdm(test_dataloader, desc=\"Generating Predictions\"):\n",
    "    with torch.no_grad():\n",
    "        input_ids = batch[\"input_ids\"].cuda()\n",
    "        attention_mask = batch[\"attention_mask\"].cuda()\n",
    "\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=128,\n",
    "            num_beams=4,\n",
    "            no_repeat_ngram_size=3,\n",
    "        )\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    for pred in decoded_preds:\n",
    "        predictions.append(pred)\n",
    "\n",
    "output_df = pd.DataFrame({'content': test_data['content'], 'tags': predictions})\n",
    "# output_df.to_csv('predictions.csv', index=False)\n",
    "output_df.to_csv('predictions.tsv', sep='\\t', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6242628,
     "sourceId": 10117771,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6244031,
     "sourceId": 10119607,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6251498,
     "sourceId": 10129791,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6261540,
     "sourceId": 10144262,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
